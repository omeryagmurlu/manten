# @package _global_

defaults:
  - _self_
  - hydra: default

  # following are global
  - experiment: ???
  - optional local: local
  - debug: null

project_name: manten

_detailed_metrics_every_n_epochs: 10
_save_every_n_epochs: 55
training:
  num_epochs: 80
  sanity_check: 5
  log_every_n_steps: 1
  save_every_n_epochs: ${_save_every_n_epochs}
  validate_every_n_epochs: ${_detailed_metrics_every_n_epochs}
  eval_train_every_n_epochs: ${_detailed_metrics_every_n_epochs}
  eval_test_every_n_epochs: ${_detailed_metrics_every_n_epochs}
  validate_ene_max_steps: 1000
  eval_train_ene_max_steps: 1000
  eval_test_ene_max_steps: 1000
  seed: ???
  deterministic: False
  resume_from_save: False
  log_aggregator:
    _target_: manten.utils.log_aggregator.LogAggregator

accelerator:
  _target_: accelerate.Accelerator
  mixed_precision: bf16
  gradient_accumulation_steps: 1
  log_with: wandb

accelerator_init_trackers:
  project_name: ${project_name}
  init_kwargs:
    wandb:
      entity: "omeryagmurlu"

_lr: 0.0003
_weight_decay: 0.005
optimizer_configurator:
  _target_: manten.utils.optimizer_configurator.OptimizerConfigurator
  default_params_config:
    lr: ${_lr}
    weight_decay: ${_weight_decay}
  params_configs:
    - contains_substrings:
        - "bias"
        - "LayerNorm.weight"
        - "LayerNorm.bias"
      lr: ${_lr}
      weight_decay: 0.0
optimizer:
  _target_: torch.optim.AdamW
  lr: ${_lr}
  weight_decay: ${_weight_decay}
lr_scheduler:
  _target_: torch.optim.lr_scheduler.ConstantLR
  factor: 1.0
