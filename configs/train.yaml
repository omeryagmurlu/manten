# @package _global_

defaults:
  - _self_
  - hydra: default

  # following are global
  - experiment: ???
  - debug: null

project_name: manten
seed: ???
deterministic: False

_save_every_n_gs: 5000
_detailed_metrics_every_n_gs: 2500
_skip_first_n_gs: 50000
training:
  sanity_check: 5
  num_epochs: .inf
  num_global_steps: 600000
  max_steps: .inf
  log_every_n_steps: 100
  save:
    every_n_global_steps: ${_save_every_n_gs}
    skip_first_global_steps: ${_skip_first_n_gs}
  val:
    every_n_global_steps: ${_detailed_metrics_every_n_gs}
    skip_first_global_steps: ${_skip_first_n_gs}
    max_steps: .inf
  eval_train: null
  eval_test:
    every_n_global_steps: ${_detailed_metrics_every_n_gs}
    skip_first_global_steps: ${_skip_first_n_gs}
    max_steps: .inf
  resume_from_save: null
  vis_metric_key: null
  log_aggregator:
    _target_: manten.utils.log_aggregator.LogAggregator
  log_train_timing: False
  custom_eval: null
  ema:
    _target_: diffusers.training_utils.EMAModel
    power: 0.75
    use_ema_warmup: True

accelerator:
  gradient_accumulation_steps: 1
  log_with: wandb

accelerator_init_trackers:
  project_name: ${project_name}
  init_kwargs:
    wandb:
      entity: "omeryagmurlu"

# _lr: 0.0003 # 3e-4
_lr: ??? #0.001 # 1e-3
_weight_decay: ??? #0.0003 #0.000006
_adamw_betas: [0.9, 0.999]
optimizer_configurator:
  _target_: manten.utils.optimizer_configurator.OptimizerConfigurator
  default_params_config:
    lr: ${_lr}
    weight_decay: ${_weight_decay}
    betas: ${_adamw_betas}
  params_configs:
    []
    # - contains_substrings:
    #     # - "bias"
    #     - "LayerNorm.weight"
    #     - "LayerNorm.bias"
    #   lr: ${_lr}
    #   weight_decay: 0.0
optimizer:
  _target_: torch.optim.AdamW
  lr: ${_lr}
  weight_decay: ${_weight_decay}
  betas: ${_adamw_betas}
lr_scheduler:
  _target_: diffusers.optimization.get_scheduler
  name: constant

hydra:
  run:
    dir: ./outputs/training/${project_name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
