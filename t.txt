TransformerForDiffusion(
  (input_emb): Linear(in_features=10, out_features=768, bias=True)
  (drop): Dropout(p=0.1, inplace=False)
  (time_emb): SinusoidalPosEmb()
  (cond_obs_emb): Linear(in_features=515, out_features=768, bias=True)
  (encoder): Sequential(
    (0): Linear(in_features=768, out_features=3072, bias=True)
    (1): Mish()
    (2): Linear(in_features=3072, out_features=768, bias=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0-11): 12 x TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (linear1): Linear(in_features=768, out_features=3072, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=3072, out_features=768, bias=True)
        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=10, bias=True)
)


MantenTransformerForDiffusion(
  (sample_emb): Linear(in_features=10, out_features=768, bias=True)
  (cond_embs): ModuleDict(
    (cond_state_embed): Linear(in_features=3, out_features=768, bias=True)
    (cond_rgb_embed): Linear(in_features=512, out_features=768, bias=True)
  )
  (cond_pos_embs): ParameterDict(
      (cond_rgb_pos_embed): Parameter containing: [torch.cuda.FloatTensor of size 1x1x768 (cuda:0)]
      (cond_state_pos_embed): Parameter containing: [torch.cuda.FloatTensor of size 1x1x768 (cuda:0)]
  )
  (time_emb): ScaledSinusoidalEmbedding()
  (post_emb_norm): Identity()
  (emb_dropout): Dropout(p=0.0, inplace=False)
  (project_emb): Identity()
  (attn_layers): Encoder(
    (layers): ModuleList(
      (0): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (1): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (2): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (3): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (4): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (5): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (6): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (7): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (8): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (9): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (10): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (11): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (12): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (13): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (14): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (15): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (16): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (17): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (18): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (19): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (20): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (21): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
      (22): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): Attention(
          (to_q): Linear(in_features=768, out_features=512, bias=False)
          (to_k): Linear(in_features=768, out_features=512, bias=False)
          (to_v): Linear(in_features=768, out_features=512, bias=False)
          (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)
          (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)
          (merge_heads): Rearrange('b h n d -> b n (h d)')
          (attend): Attend(
            (attn_dropout): Dropout(p=0.0, inplace=False)
          )
          (to_out): Linear(in_features=512, out_features=768, bias=False)
        )
        (2): Residual()
      )
      (23): ModuleList(
        (0): ModuleList(
          (0): LayerNorm(
            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
          )
          (1-2): 2 x None
        )
        (1): FeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=768, out_features=3072, bias=True)
              (1): GELU(approximate='none')
            )
            (1): Dropout(p=0.0, inplace=False)
            (2): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): Residual()
      )
    )
    (layer_integrators): ModuleList(
      (0-23): 24 x None
    )
    (adaptive_mlp): Identity()
    (final_norm): LayerNorm(
      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=False)
    )
    (skip_combines): ModuleList(
      (0-23): 24 x None
    )
  )
  (to_logits): Linear(in_features=768, out_features=10, bias=False)
)